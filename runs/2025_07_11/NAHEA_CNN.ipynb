{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8a41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from qutip.solver.floquet import progress_bars\n",
    "import torch\n",
    "from pulser import Pulse, Register, Sequence\n",
    "from pulser.devices import MockDevice\n",
    "from pulser_diff.backend import TorchEmulator\n",
    "from pyqtorch.utils import SolverType\n",
    "from torch import Tensor\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dbb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d7c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.NAHEA_CNN import NAHEA_CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38c41dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4000, 18)\n",
      "y_train shape: (4000, 1)\n",
      "X_test shape: (1000, 18)\n",
      "y_test shape: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data from sin dataset\n",
    "data_save_dir = Path(\"../../data\") / \"sin\"\n",
    "data_save_path_train = data_save_dir / \"train.h5\"\n",
    "data_save_path_test = data_save_dir / \"test.h5\"\n",
    "\n",
    "with h5.File(data_save_path_train, \"r\") as f:\n",
    "    X_train = f[\"X\"][:].squeeze(2)\n",
    "    y_train = f[\"y\"][:]\n",
    "\n",
    "with h5.File(data_save_path_test, \"r\") as f:\n",
    "    X_test = f[\"X\"][:].squeeze(2)\n",
    "    y_test = f[\"y\"][:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f0699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 150\n",
    "n_test = 500 # testing is much faster than training\n",
    "X_train = X_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "X_test = X_test[:n_test]\n",
    "y_test = y_test[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    plt.plot(X_train[i], label=f\"Sample {i+1}, f= {y_train[i][0]:.2f}\")\n",
    "plt.title(\"Generated Sin Wave Data Samples\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c137f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final FC NN: Sequential(\n",
      "  (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n",
      "Adding parameter 0.weight to model\n",
      "Adding parameter 0.bias to model\n",
      "Adding parameter 2.weight to model\n",
      "Adding parameter 2.bias to model\n",
      "Adding parameter 4.weight to model\n",
      "Adding parameter 4.bias to model\n"
     ]
    }
   ],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "hparams = {\n",
    "    \"n_features\": (n_features := 5),  # =1D kernel size = number of qubits\n",
    "    \"sampling_rate\": 0.2,\n",
    "    \"protocol\": \"min-delay\",\n",
    "    \"n_ancilliary_qubits\": (n_ancilliary_qubits := 0), # not implemented\n",
    "    \"input_length\": seq_len,\n",
    "    \"stride\": 3,  # stride for the convolution\n",
    "    \"output_dim\": 1,  # output dimension of the final FC NN\n",
    "    \"hidden_layers_dims\": [10, 5],\n",
    "}\n",
    "\n",
    "sep = 7.0\n",
    "parameters = {\n",
    "    # separation of 7 between the qubits\n",
    "    \"positions\": [[sep * i - (sep * 2), 0] for i in range(n_features)],\n",
    "    \"local_pulses_omega_1\": [0.5, 1.0, 1.5, 1.0, 0.5],\n",
    "    \"local_pulses_delta_1\": [0.0] * n_features,\n",
    "    \"global_pulse_omega_1\": 1.0,\n",
    "    \"global_pulse_delta_1\": 0.0,\n",
    "    \"global_pulse_omega_2\": 0.5,\n",
    "    \"global_pulse_delta_2\": 0.0,\n",
    "    \"global_pulse_duration\": 100,\n",
    "    \"local_pulse_duration\": 80,\n",
    "    \"embed_pulse_duration\": 80,\n",
    "}\n",
    "\n",
    "\n",
    "NAHEA_CNN = NAHEA_CNN_1(\n",
    "    hparams=hparams, parameters=parameters, name=\"NAHEA_CNN_2\"\n",
    ")\n",
    "\n",
    "# set some parameters to not require gradients. They didn't change much last time\n",
    "NAHEA_CNN._parameters[\"positions\"].requires_grad = False\n",
    "NAHEA_CNN._parameters[\"local_pulses_delta_1\"].requires_grad = False\n",
    "NAHEA_CNN._parameters[\"global_pulse_delta_1\"].requires_grad = False\n",
    "NAHEA_CNN._parameters[\"global_pulse_delta_2\"].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98a78fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positions': tensor([[-14.,   0.],\n",
      "        [ -7.,   0.],\n",
      "        [  0.,   0.],\n",
      "        [  7.,   0.],\n",
      "        [ 14.,   0.]]), 'local_pulses_omega_1': tensor([0.5000, 1.0000, 1.5000, 1.0000, 0.5000], requires_grad=True), 'local_pulses_delta_1': tensor([0., 0., 0., 0., 0.]), 'global_pulse_omega_1': tensor(1., requires_grad=True), 'global_pulse_delta_1': tensor(0.), 'global_pulse_omega_2': tensor(0.5000, requires_grad=True), 'global_pulse_delta_2': tensor(0.), 'global_pulse_duration': tensor(100.), 'local_pulse_duration': tensor(80.), 'embed_pulse_duration': tensor(80.), 'conv_params': tensor([ 0.0764, -0.0013,  0.0203,  0.0648,  0.0537,  0.0020,  0.0314,  0.0290,\n",
      "         0.0410,  0.0607,  0.0506,  0.0291,  0.0456,  0.0693,  0.0362,  0.0246,\n",
      "         0.0307,  0.0383,  0.0507,  0.0291,  0.0379,  0.0539,  0.0304,  0.0382,\n",
      "        -0.0052,  0.0123,  0.0486,  0.0341,  0.0585,  0.0110,  0.0576,  0.0125],\n",
      "       dtype=torch.float64, requires_grad=True), '0.weight': Parameter containing:\n",
      "tensor([[-0.1093,  0.3929,  0.4770,  0.3689, -0.0763],\n",
      "        [-0.5964, -0.3813,  0.4171, -0.4968,  0.4913],\n",
      "        [ 0.1728,  0.5729,  0.0025,  0.1810, -0.4648],\n",
      "        [-0.5345, -0.2909,  0.2870,  0.1349, -0.5016],\n",
      "        [-0.4412, -0.1674, -0.2568,  0.6035,  0.5457],\n",
      "        [ 0.0085, -0.4430,  0.5142, -0.4567, -0.4291],\n",
      "        [-0.6062, -0.4004,  0.3028,  0.6154, -0.5292],\n",
      "        [ 0.1049, -0.0044,  0.4265, -0.1794, -0.2385],\n",
      "        [ 0.6087,  0.4586,  0.2563, -0.2487,  0.5857],\n",
      "        [ 0.3436,  0.3034,  0.6027, -0.3132,  0.1965]], dtype=torch.float64,\n",
      "       requires_grad=True), '0.bias': Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
      "       requires_grad=True), '2.weight': Parameter containing:\n",
      "tensor([[ 0.5582,  0.3146,  0.5303, -0.2042, -0.0892, -0.2159, -0.5297, -0.5073,\n",
      "         -0.4372, -0.0111],\n",
      "        [-0.1746,  0.5012,  0.5025, -0.5656, -0.1290, -0.1757, -0.6148, -0.4927,\n",
      "          0.2474, -0.1149],\n",
      "        [-0.1846, -0.4729,  0.1933,  0.2767, -0.0397,  0.3449,  0.2339, -0.2286,\n",
      "          0.1440, -0.4901],\n",
      "        [-0.2268, -0.1561, -0.2036,  0.4120, -0.2046, -0.3347,  0.2912, -0.4232,\n",
      "          0.2839,  0.2045],\n",
      "        [ 0.1465,  0.1372,  0.2429,  0.3334,  0.3184,  0.4234,  0.5488, -0.3472,\n",
      "         -0.3420,  0.5390]], dtype=torch.float64, requires_grad=True), '2.bias': Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64, requires_grad=True), '4.weight': Parameter containing:\n",
      "tensor([[ 0.3237,  0.9276,  0.2030, -0.5078, -0.6127]], dtype=torch.float64,\n",
      "       requires_grad=True), '4.bias': Parameter containing:\n",
      "tensor([0.], dtype=torch.float64, requires_grad=True)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203376953a4c4374a483eb88d7277c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 loss: 0.5207, RMSE: 0.7216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train[i : i \u001b[38;5;241m+\u001b[39m batch_size], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m     26\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     27\u001b[0m     y_train[i : i \u001b[38;5;241m+\u001b[39m batch_size], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m     28\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m batch_out \u001b[38;5;241m=\u001b[39m [NAHEA_CNN(xx) \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m x_batch]\n\u001b[1;32m     30\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([bo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m bo \u001b[38;5;129;01min\u001b[39;00m batch_out])\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predicted\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), y_batch)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train[i : i \u001b[38;5;241m+\u001b[39m batch_size], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m     26\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     27\u001b[0m     y_train[i : i \u001b[38;5;241m+\u001b[39m batch_size], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m     28\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m batch_out \u001b[38;5;241m=\u001b[39m [\u001b[43mNAHEA_CNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m x_batch]\n\u001b[1;32m     30\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([bo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m bo \u001b[38;5;129;01min\u001b[39;00m batch_out])\n\u001b[1;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predicted\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), y_batch)\n",
      "File \u001b[0;32m~/Documents/Projects/Pasqal/NeutralAtomEfficientEncoding/runs/2025_07_11/../../source/NAHEA.py:84\u001b[0m, in \u001b[0;36mNAHEA.__call__\u001b[0;34m(self, x, **args)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/Pasqal/NeutralAtomEfficientEncoding/runs/2025_07_11/../../source/NAHEA_CNN.py:304\u001b[0m, in \u001b[0;36mNAHEA_CNN_1.forward\u001b[0;34m(self, x, time_grad, dist_grad, solver)\u001b[0m\n\u001b[1;32m    302\u001b[0m sim \u001b[38;5;241m=\u001b[39m TorchEmulator\u001b[38;5;241m.\u001b[39mfrom_sequence(seq_built, sampling_rate\u001b[38;5;241m=\u001b[39msampling_rate)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 304\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSolverType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDP5_SE\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pulser_diff/backend.py:548\u001b[0m, in \u001b[0;36mTorchEmulator.run\u001b[0;34m(self, time_grad, dist_grad, solver, **options)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnoise)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[1;32m    533\u001b[0m     {\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdephasing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;66;03m# If there is \"SPAM\", the preparation errors must be zero\u001b[39;00m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPAM\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnoise \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meta \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 548\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;66;03m# Stores the different initial configurations and frequency\u001b[39;00m\n\u001b[1;32m    552\u001b[0m         initial_configs \u001b[38;5;241m=\u001b[39m Counter(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mruns)\n\u001b[1;32m    561\u001b[0m         )\u001b[38;5;241m.\u001b[39mmost_common()\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pulser_diff/backend.py:488\u001b[0m, in \u001b[0;36mTorchEmulator.run.<locals>._run_solver\u001b[0;34m()\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns CoherentResults: Object containing evolution results.\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [SolverType\u001b[38;5;241m.\u001b[39mDP5_SE, SolverType\u001b[38;5;241m.\u001b[39mKRYLOV_SE]:\n\u001b[0;32m--> 488\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msesolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hamiltonian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hamiltonian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpsi0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtsave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_times_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m SolverType\u001b[38;5;241m.\u001b[39mDP5_ME:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnoise:\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pyqtorch/time_dependent/sesolve.py:52\u001b[0m, in \u001b[0;36msesolve\u001b[0;34m(H, psi0, tsave, solver, options)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested solver is not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# compute the result\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Result(result)\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pyqtorch/time_dependent/integrators/adaptive.py:192\u001b[0m, in \u001b[0;36mAdaptiveIntegrator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tnext \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtsave:\n\u001b[0;32m--> 192\u001b[0m     y, \u001b[38;5;241m*\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtnext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m    194\u001b[0m     t \u001b[38;5;241m=\u001b[39m tnext\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pyqtorch/time_dependent/integrators/adaptive.py:86\u001b[0m, in \u001b[0;36mAdaptiveIntegrator.integrate\u001b[0;34m(self, t0, t1, y, *args)\u001b[0m\n\u001b[1;32m     83\u001b[0m     dt \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# compute the next step\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m ft_new, y_new, y_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mode_fun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_error(y_err, y, y_new)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pyqtorch/time_dependent/methods/dp5.py:81\u001b[0m, in \u001b[0;36mDormandPrince5.step\u001b[0;34m(self, t, y, f, dt, fun)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m     80\u001b[0m     dy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensordot(dt \u001b[38;5;241m*\u001b[39m beta[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :i], k[:i]\u001b[38;5;241m.\u001b[39mclone(), dims\u001b[38;5;241m=\u001b[39m([\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 81\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     k[i] \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39muse_sparse \u001b[38;5;28;01melse\u001b[39;00m a\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# compute results\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pyqtorch/time_dependent/solvers.py:34\u001b[0m, in \u001b[0;36mSESolver.ode_fun\u001b[0;34m(self, t, psi)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# filter-out UserWarning about \"Sparse CSR tensor support is in beta state\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m     res \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39mj \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m (psi\u001b[38;5;241m.\u001b[39mto_sparse() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39muse_sparse \u001b[38;5;28;01melse\u001b[39;00m psi)\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/envs/3.10/lib/python3.10/site-packages/pulser_diff/hamiltonian.py:544\u001b[0m, in \u001b[0;36mHamiltonian.build_ham_tensor.<locals>.H_t\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    542\u001b[0m     amp \u001b[38;5;241m=\u001b[39m amp_val[t_idx1] \u001b[38;5;241m+\u001b[39m (amp_val[t_idx2] \u001b[38;5;241m-\u001b[39m amp_val[t_idx1]) \u001b[38;5;241m*\u001b[39m (t \u001b[38;5;241m-\u001b[39m t_idx1 \u001b[38;5;241m*\u001b[39m dt) \u001b[38;5;241m/\u001b[39m dt\n\u001b[1;32m    543\u001b[0m     ham_mat \u001b[38;5;241m=\u001b[39m amp_mat \u001b[38;5;241m*\u001b[39m amp\n\u001b[0;32m--> 544\u001b[0m     ham \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ham_mat \u001b[38;5;241m+\u001b[39m \u001b[43mham_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tensor, ham)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "loss_hist = []\n",
    "# MSE loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "NAHEA_CNN.train()\n",
    "print(f\"{NAHEA_CNN.parameters()}\")\n",
    "optimizer = torch.optim.Adam(\n",
    "    [param for _, param in NAHEA_CNN.parameters().items() if param.requires_grad],\n",
    "    lr=0.01,\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "params_hist = [copy.deepcopy(NAHEA_CNN.parameters())]\n",
    "for epoch in range(epochs):\n",
    "    # shuffle X_train and y_train\n",
    "    idxs = np.arange(len(X_train))\n",
    "    idxs = np.random.permutation(idxs)\n",
    "    X_train = X_train[idxs]\n",
    "    y_train = y_train[idxs]\n",
    "\n",
    "    epoch_losses = []\n",
    "    for i in tqdm(range(0, len(X_train), batch_size), desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        x_batch = torch.tensor(X_train[i : i + batch_size], dtype=torch.float64)\n",
    "        y_batch = torch.tensor(\n",
    "            y_train[i : i + batch_size], dtype=torch.float64\n",
    "        ).squeeze(1)\n",
    "        batch_out = [NAHEA_CNN(xx) for xx in x_batch]\n",
    "        predicted = torch.stack([bo[\"output\"] for bo in batch_out])\n",
    "        loss = loss_fn(predicted.squeeze(1), y_batch)\n",
    "        tqdm.write(\n",
    "            f\"Batch {i // batch_size + 1} loss: {loss.item():.4f}, \"\n",
    "            f\"RMSE: {np.sqrt(loss.item()):.4f}\"\n",
    "        )\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss_hist.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        params_hist.append(copy.deepcopy(NAHEA_CNN.parameters()))\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    tqdm.write(f\"Epoch {epoch+1} train loss: {epoch_loss}\")\n",
    "loss_hist = np.array(loss_hist)\n",
    "print(f\"Final loss: {loss_hist[-1]}\")\n",
    "print(f\"Final loss (RMSE): {np.sqrt(loss_hist[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "872bc526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss MSE: 0.0002715730975308407\n",
      "RMSE: 0.016479475038084213\n",
      "final parameters:\n",
      "{'positions': tensor([[-13.9168,   0.0000],\n",
      "        [ -7.0230,   0.0000],\n",
      "        [  0.0461,   0.0000],\n",
      "        [  7.0605,   0.0000],\n",
      "        [ 13.9281,   0.0000]], requires_grad=True), 'local_pulses_omega_1': tensor([0.3825, 1.1201, 1.5070, 1.1189, 0.4130], requires_grad=True), 'local_pulses_delta_1': tensor([0., 0., 0., 0., 0.], requires_grad=True), 'global_pulse_omega_1': tensor(1.1278, requires_grad=True), 'global_pulse_delta_1': tensor(0., requires_grad=True), 'global_pulse_omega_2': tensor(0.4517, requires_grad=True), 'global_pulse_delta_2': tensor(0., requires_grad=True), 'global_pulse_duration': tensor(100.), 'local_pulse_duration': tensor(80.), 'embed_pulse_duration': tensor(80.), 'conv_params': tensor([ 0.0344, -0.1318,  0.1366, -0.1026,  0.1580,  0.2043,  0.0647, -0.1711,\n",
      "         0.2014,  0.1954, -0.0833, -0.1819,  0.1011,  0.1543, -0.0161,  0.3425,\n",
      "         0.0115, -0.0572, -0.0357,  0.0202,  0.1883,  0.0221,  0.2017,  0.1189,\n",
      "        -0.0858,  0.1282, -0.1832, -0.0534, -0.0454,  0.1713,  0.3058,  0.2839],\n",
      "       dtype=torch.float64, requires_grad=True), '0.weight': Parameter containing:\n",
      "tensor([[-4.2671e-02, -6.8848e-01,  5.2302e-01,  3.3012e-01,  1.6610e-01],\n",
      "        [-3.4243e-02,  3.7886e-01, -1.8640e-01, -1.4824e-01, -2.8699e-01],\n",
      "        [ 1.5618e-05, -5.8271e-06, -8.9009e-06,  8.9247e-06,  1.1795e-05],\n",
      "        [ 3.7576e-01,  6.3347e-02,  5.3743e-01, -1.6474e-01, -2.3593e-01],\n",
      "        [ 5.6905e-01,  3.0743e-01,  4.0704e-01,  3.0546e-01,  8.1827e-02],\n",
      "        [ 2.2720e-01,  5.0645e-01,  6.2811e-02,  1.1126e-01,  6.3443e-01],\n",
      "        [-1.2654e-01,  4.6276e-01,  9.8360e-02, -6.5204e-01, -1.3556e-01],\n",
      "        [-3.5374e-02, -7.0395e-02, -5.2489e-02, -4.0906e-02, -7.4593e-02],\n",
      "        [ 1.7007e-01, -7.1637e-02, -1.1496e-01, -1.9355e-01, -2.1948e-01],\n",
      "        [-3.2751e-01,  6.7157e-01, -3.1030e-01,  3.9272e-01,  3.2884e-01]],\n",
      "       dtype=torch.float64, requires_grad=True), '0.bias': Parameter containing:\n",
      "tensor([-0.0906, -0.0510, -0.0137,  0.0509, -0.0088,  0.0835, -0.1002, -0.0692,\n",
      "        -0.1085,  0.0186], dtype=torch.float64, requires_grad=True), '2.weight': Parameter containing:\n",
      "tensor([[-4.8742e-03,  1.5855e-01,  2.6034e-05, -1.0179e-02,  6.5079e-01,\n",
      "          1.4485e-01, -3.8197e-01,  1.0656e-01,  3.3871e-03, -9.6145e-02],\n",
      "        [ 2.0071e-01,  5.8824e-07,  1.4627e-06, -4.9498e-01,  1.9087e-01,\n",
      "         -5.2810e-01, -2.0862e-01,  2.7255e-05,  1.0149e-06,  4.1591e-01],\n",
      "        [-4.8002e-01, -1.2635e-02,  1.5151e-05,  3.5906e-01,  5.5833e-01,\n",
      "          7.2347e-02, -1.1886e-01,  8.4251e-02,  1.1194e-04,  6.2660e-01],\n",
      "        [ 1.5617e-01, -1.4324e-01,  1.4642e-05, -2.4065e-01,  5.5994e-01,\n",
      "         -3.3756e-01,  4.7629e-01,  1.6896e-06,  6.7292e-06, -7.2763e-02],\n",
      "        [-4.5033e-02, -1.0790e-06, -1.7848e-06,  1.3976e-01, -3.8427e-02,\n",
      "         -2.0061e-01, -1.9294e-06, -2.0830e-06,  2.0565e-07, -7.2364e-02]],\n",
      "       dtype=torch.float64, requires_grad=True), '2.bias': Parameter containing:\n",
      "tensor([ 0.0607, -0.1091,  0.0581, -0.0855, -0.0435], dtype=torch.float64,\n",
      "       requires_grad=True), '4.weight': Parameter containing:\n",
      "tensor([[ 0.9194, -0.0849,  0.4872, -0.5885, -0.2480]], dtype=torch.float64,\n",
      "       requires_grad=True), '4.bias': Parameter containing:\n",
      "tensor([0.0634], dtype=torch.float64, requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "NAHEA_CNN.eval()\n",
    "y_pred_test = []\n",
    "for i in range(len(X_test)):\n",
    "    x_test = torch.tensor(X_test[i], dtype=torch.float64)\n",
    "    pred = NAHEA_CNN(x_test)[\"output\"].item()\n",
    "    y_pred_test.append(pred)\n",
    "y_pred_test = torch.tensor(y_pred_test, dtype=torch.float64)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float64).squeeze(1)\n",
    "loss_test = loss_fn(y_pred_test, y_test)\n",
    "print(f\"Test loss MSE: {loss_test.item()}\")\n",
    "print(f\"RMSE: {torch.sqrt(loss_test).item()}\")\n",
    "print(\"final parameters:\")\n",
    "print(NAHEA_CNN.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_test.numpy(), label=\"True Frequencies\", marker='o')\n",
    "plt.plot(y_pred_test.numpy(), label=\"Predicted Frequencies\", marker='x')\n",
    "plt.title(\"True vs Predicted Frequencies\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e633f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = y_test - y_pred_test\n",
    "# histogram of the differences\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(diffs.numpy(), bins=20, alpha=0.7)\n",
    "plt.title(\"Histogram of Differences (True - Predicted Frequencies)\")\n",
    "plt.xlabel(\"Difference\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30689aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist, label=\"Training Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0107ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['positions', 'local_pulses_omega_1', 'local_pulses_delta_1', 'global_pulse_omega_1', 'global_pulse_delta_1', 'global_pulse_omega_2', 'global_pulse_delta_2', 'global_pulse_duration', 'local_pulse_duration', 'embed_pulse_duration', 'conv_params', '0.weight', '0.bias', '2.weight', '2.bias', '4.weight', '4.bias'])\n"
     ]
    }
   ],
   "source": [
    "params_hist_nice = {\n",
    "    key: np.array([i[key].detach().numpy() for i in params_hist])\n",
    "    for key in params_hist[0].keys()\n",
    "}\n",
    "print(params_hist_nice.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef993e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_hist = np.array([i[\"positions\"].detach().numpy() for i in params_hist])\n",
    "local_pulses_omega_1_hist = np.array([i[\"local_pulses_omega_1\"].detach().numpy() for i in params_hist])\n",
    "local_pulses_delta_1_hist = np.array([i[\"local_pulses_delta_1\"].detach().numpy() for i in params_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41495cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "plt.sca(axs[0])\n",
    "for qubit in range(positions_hist.shape[1]):\n",
    "    plt.plot(positions_hist[:, qubit, 0], positions_hist[:, qubit, 1], label=f\"Qubit {qubit+1}\")\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.title(\"Qubit Positions Over Training (not much movement)\")\n",
    "plt.legend()\n",
    "plt.sca(axs[1])\n",
    "# only first qubit\n",
    "plt.plot(positions_hist[:, 0, 0], label=\"Qubit 1\")\n",
    "plt.plot(positions_hist[:, 1, 0], label=\"Qubit 2\")\n",
    "plt.plot(positions_hist[:, 2, 0], label=\"Qubit 3\")\n",
    "plt.plot(positions_hist[:, 3, 0], label=\"Qubit 4\")\n",
    "plt.plot(positions_hist[:, 4, 0], label=\"Qubit 5\")\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.title(\"Qubit Positions Over Training\")\n",
    "plt.legend()\n",
    "plt.tight_layout()  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_keys = ['local_pulses_omega_1', 'local_pulses_delta_1', 'global_pulse_omega_1', 'global_pulse_delta_1', 'global_pulse_omega_2', 'global_pulse_delta_2']\n",
    "fig, axs = plt.subplots(len(plot_keys), 1, figsize=(10, 4 * len(plot_keys)))\n",
    "for i, key in enumerate(plot_keys):\n",
    "    axs[i].plot(params_hist_nice[key])\n",
    "    axs[i].set_title(key)\n",
    "    axs[i].set_xlabel(\"Training Step\")\n",
    "    axs[i].set_ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ddb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# number of learned parameters, i.e. with require_grad=True\n",
    "NAHEA_CNN.train()\n",
    "# number of parameters\n",
    "n_params = sum(v.numel() for p, v in NAHEA_CNN.parameters().items() if v.requires_grad)\n",
    "print(f\"Number of trainable parameters: {n_params}\")\n",
    "# save training history\n",
    "os.makedirs(\"losses_hist\", exist_ok=True)\n",
    "with open(\"losses_hist/NAHEA_CNN_train_hist.pickle\", \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"loss_hist\": loss_hist,\n",
    "            \"params_hist\": params_hist,\n",
    "            \"hparams\": hparams,\n",
    "            loss_test.item(): \"test_loss\",\n",
    "            \"n_learned_parameters\": n_params,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
